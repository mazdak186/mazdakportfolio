---
title: "Fermi National Accelerator Lab’s Hazardous Maintenance Solution Using Object Detection in 3D Space"
date: 2020-04-15T15:34:30-04:00
categories:
  - Capstone Project
tags:
  - Machine Learning
  - Python
toc: true
toc_label:
toc_icon: 'bars'
classes: wide
excerpt: "Capstone Project for UIC"
sidebar:
  nav: "capstone"

---

# Overview
When faced with a business challenge that may require a machine learning solution, we must consider the following. What is the machine learning problem type? What are the data sources? How will the output be used? What metrics will deem the model successful? How will the finished model be properly implemented? Is machine learning even required to effectively solve the problem? These are some of the questions my colleagues and I had to ask ourselves for our University of Illinois capstone project when we were tasked with exploring the first machine learning implementation at Fermi National Accelerator Laboratory (Fermilab). 
## Fermilab's Problem
Fermilab has a particle accelerator that fires high power neutrinos at a detector to determine its mass. This means that the accelerator itself becomes too radioactive for maintenance workers to easily work on it. They currently need to use hot cells with mechanical arms and PPE to perform maintenance on the accelerator. This process takes a long time and requires extensive training for the workers. Even with PPE the workers are still at risk of accumulating radiation over time so implementing a ML approach to maintenance will not only save Fermilab time and money but also help promote the health and safety of its workers.
## Proposed Solution
One of the first steps of any maintenance work on the accelerator is to remove any fasteners to gain access inside the machine. Therefore, it would be helpful for a robotic arm to automatically remove these fasteners without the need of a human worker. In order to achieve this, we need to implement a computer vision algorithm that would be able to locate these fasteners in three-dimensional space. The robot will "see" the fastener using a 3D camera and will calculate the coordinates of said fastener in order to travel that distance and remove it.
## Machine Learning Algorithms
The task of "seeing" an object in a video feed requires using a computer vision solution. There are several computer vision technologies that do not utilize machine learning, but they are only applicable  in a very uniform environment like a factory's assembly line where a camera would not have much variation in the images it sees. For our situation there are varying angles, lighting conditions, backgrounds, colors, and even types of fasteners on the accelerator. In order to filter through all the unnecessary information to locate a nut or bolt in an image we will use a type of machine learning called object detection. Object detection utilizes a type of machine learning called deep learning which relies on big datasets and powerful hardware for the learning process. We will be supervising the learning process which means we will feed the model large quantities of images of nuts and bolts that are labeled accordingly. This constrasts with unsupervised learning which lets the algorithm create its own classifications depending on the features it extracts from the dataset. That is not useful for us because we already know what kind of fasteners exist on the accelerator. Deep neural networks have multiple hidden layers between the input and output that extract complex features a human would never be able to find. The model will learn these features specific to a nut and a bolt making it so it should not matter where the camera is positioned or what part of the accelerator the camera is pointed at.
## Assumptions
Before moving to the design process assumptions must be established to narrow the scope of the project. By setting limits to which the project is conducted, a higher success rate for the project is established for the given timeline. 
The first assumption made is that the device will be comprised of a microcontroller in order to fulfill the design requirements of being mountable on a robotic arm and capable of enclosing all necessary software. It must have enough storage capacity to house all necessary libraries and packages that will be used to create a machine learning and computer vision system. The two microcontrollers that will be considered for the project are the Nvidia Jetson Nano and Nvidia Jetson TX2.
Another assumption for the project is that the camera to be used is a 3D stereo camera. This assumption is made due to the design requirement that the device must be able to determine the location and orientation of the previously identified fasteners to a common coordinate origin. Furthermore, the underlying principle of deep learning and using CNNs is based on the human brain and human experience and the human experience is based on a three-dimensional reality. Therefore, having a 3D stereo camera collecting data on the environment would reflect a more accurate image of the camera’s surroundings and allow for the creation of a virtual environment to base a coordinate system about which a robotic arm can be adjusted to move towards the nuts and bolts detected by the camera in 3D space. It can be concluded that by using both CNNs and 3D stereo camera, the vision system would be able to create a more accurate model of reality which would then allow the entire system to be more accurate.
Since the goal of the machine learning device is to perform maintenance and end-of-life procedures on highly radioactive components from an accelerator beamline enclosure, it can be assumed that the device attached to the robot arm will only be used once before being becoming obsolete and having to be thrown away with all other contaminated parts of the beamline and maintenance devices. Therefore, the device, software and hardware must be easily reproducible for future maintenance projects of similar scope. This can be done so by backing up the system to a separate hard drive that can be uploaded onto other Nvidia Jetson devices as the project uses various scripts and programs that are not unique to the device that will be presented in this project. 
Finally, it is assumed that transfer learning will be used to create he machine learning algorithm. Many advanced architectures have already been created and published for tasks of relatively similar scopes to this project. So a pre-trained model will be chosen based on accuracy and inference time, and transfer learning will be applied by means of creating a unique dataset of nuts and bolts and training the premade model to detect the target objects from the project's dataset.
## Final Design
After extensive research it can be determined that though many different machine learning applications exist in the realm of image classification, object detection and image segmentation, nothing has yet to be applied to the specific task and environment for this project. From this, a decision matrix, Table 1, was created for choosing between the Jetson Nano and the Jetson TX2 as the microcontroller for the project. The criteria of memory capacity as well as compatibility with the two provided 3D stereo cameras were given the highest weight of 5 as these are key for moving forward with the project. The storage capacity was given a weight of 3 as memory is more vital to the project compared to storage. The cost was given a low weight of 2 as both devices were provided either by UIC or by and Fermilab. The size was given a weight of 4 as Fermilab wants the device to be able to be connected to a robotic arm.

<br />

|![image](/assets/images/table1.png)|
|:--:|
|*Table 1: Decision Matrix for microcontroller*|

<br />

Next a decision matrix, Table 2, was made to compare the different types of cameras that may be used for the object detection. The resolution was given the highest weight of 5 as this is most important when having the system detect the target objects. The cost was given a weight of 2 as the cameras were all provided by either Fermilab or fell within the budget for UIC for the project. Compatibility was given a weight of 3 because the camera chosen must be compatible with the Jetson interface, however, most cameras are compatible, so this was not a problem to overcome. The size was also given a weight of 3 as the camera will be attached to a robot arm and must be able to move with the robot arm without inhibiting the motion

<br />

|![image](/assets/images/table2.png)|
|:--:|
|*Table 2: Decision Matrix for camera*|

<br />

Next a decision matrix, Table 3, was made to compare the different types of model architectures that may be used for training the custom machine learning model using transfer learning. The average recall and precision were given the highest weights of 10 each as these are equally important when having the system correctly detect and differentiate between the target objects. The speed of detecting the objects was given a weight of 5 because time is not as important as accuracy of detection for maintenance work.

<br />

|![image](/assets/images/table3.png)|
|:--:|
|*Table 3: Decision Matrix for model architecture*|

<br />

Taking the above assumptions and decision matrices scores into consideration, this leads to the following proposed design of the system for our project. The SSD Inception V2 model architecture will be used to train the custom model, while the Nvidia Jetson Nano will be used as the microcontroller for running the pretrained object detection and classification script via the Intel Realsense D435 depth camera. However, as the Jetson Nano does not have the RAM compacity to perform the training of the model, a local directory on the UIC supercomputer, Dragon, will be set up to be used to train our model using transfer learning. Once the platform is created and a model is trained, a custom detection script will be created for detecting nuts and bolts on a flange and displaying the 3D coordinate location of each target object.


# Dataset Creation
After observing the results of many different training sessions on the supercomputer, it became clear that the accuracy and precision of the model are incredibly dependent on the images that comprise the inputted dataset. The first dataset was made up of 231 images taken from an iPhone of a variety of single nuts and single bolts in each image. An example of an image from the dataset is shown below in Figure 1.

<br />

|![image](/assets/images/data1.png)|
|:--:|
|*Figure 1. Example of image of the back of the 3D printed flange*|

<br />

Due to the resizing of the image for use in TensorFlow, the hex nut in Figure 1 is flattened and not ideal for machine learning. Since the machine learning and computer vision system will ultimately be applied to the window flange of the NuMI target and since the dataset was deemed unsatisfactory, it was concluded that a new dataset was to be created using the prototype flange. 
The second dataset was made of around 800 images also taken from an iPhone of the 3D printed flange shown in Figure 1 in the Technical Drawings section of the report. This flange, like the window flange of the NuMI target, is made up of 32 nuts and 42 bolts. The pictures taken showed varied light and background settings and generally displayed the entire flange at varying distances from the camera, an example is shown in Figure 2 below.

<br />

|![image](/assets/images/data2.png)|
|:--:|
|*Figure 2. Example of image of the flange from third dataset*|

<br />

The iPhone camera was set to take square images so that when resizing the images occurred, they would not be distorted. However, after observing the results of the model training on TensorBoard, it was concluded that the images contained too many nuts and bolts for the system to accurately detect any of the target objects. Also, as seen in Figure 2, the lighting conditions were not permissible to properly identifying the nuts and bolts, and the images were captured at too great of a distance from the flange.
	Considering the results from the first two datasets, a third dataset was created by taking 1200 images of nuts and bolts on various flanges found around the city of Chicago with the hopes that the varying backgrounds may improve training accuracy. These flanges had a more limited number of nuts and bolts attached to them – ranging between two and four of one of the types of fasteners. Due to many of the flanges being outside, the lighting was significantly better than overhead lighting. However, after reviewing the training results in TensorBoard, it was concluded that the presence background noise and the appearance of partial nuts and bolts, seen in Figure 3, confused the model and resulted in poor accuracy and precision.
	
<br />

|![image](/assets/images/data3.png)|
|:--:|
|*Figure 3. Example of image of the flange from third dataset*|

<br />

After consulting with Dr. Yurkiv, the next dataset was to be created from the rendering of the SolidWorks assembly file of flanges found at McMaster Carr limited to four nuts and bolts per flange. A dataset of 1400 labeled images was created from four different flanges, an example is presented below in Figure 4. This led to the greatest accuracy and precision results of the datasets thus far. However, because the model was only trained to identify nuts and bolts with a blank background, this led to false detections of target objects when an image in the testing folder happened to have different features in the background, as shown in Figure 5.

<br />

|![image](/assets/images/data4.png)|
|:--:|
|*Figure 4. Example of images of the flange from SolidWorks dataset with no background*|

<br />

<br />

|![image](/assets/images/data5.png)|
|:--:|
|*Figure 5. Ground truth (right) is compared to detections (left) where the model confuses the grass for a nut with a classification score of 72%*|

<br />

Though grass will not be found in Fermilab, more images of the rendered SolidWorks assembly were taken with varying backgrounds found in SolidWorks. The final dataset was therefore made up of 2361 CAD images from SolidWorks with 1405 of the images without backgrounds and 956 images with varied background as shown below in Figure 6. The results from training the model based on the newest dataset led to the best accuracy and precision results of all the datasets.

<br />

|![image](/assets/images/data6.png)|
|:--:|
|*Figure 6. Example image from final dataset using a background with varying features*|

<br />

By using SolidWorks to render a model of a flange, a camera path can be set to save images along the pathway. This not only saves time from not taking individual images manually, but also allows for an infinite number of pathways to be set to acquire a full range of angles of the flange to be captured for training the model. Ultimately, this will automate the data collection process for the backend of model training, especially as the accuracy and precision of the model increased with the use of this type of dataset.

## Converting a Set of Images to a TFRecord
Tensorflow optimizes the dataset import pipeline by converting large datasets into a type of binary storage format known as a TFRecord file. To do this, each image must have a bounding box around the target object and must have a corresponding .xml file that has the object classification and the bounding box x-y coordinates. Using LabelImg, an image labeling software for machine learning, each fastener in the images was labeled by drawing a box around the target object and giving the labels ‘nut’ or ‘bolt.’ The software automatically creates a .xml file for each image containing the information that is to be read by TensorFlow. The images and their corresponding .xml files are then split into two folders, ‘test’ and ‘train’ containing 20% and 80% of the files, respectively. To convert the files, the following Python script is run and points to the paths of the dataset with the images and their corresponding .xml files and outputs .record files.

```python
import os
import xml.etree.ElementTree as ET
import tensorflow as tf
from object_detection.utils import dataset_util



# This are the path to the datasets and to the output files.
# NEED TO BE UPDATED IN CASE THE DATASET CHANGES
PATH_TEST = "lDataset/test/"
PATH_RECORD_TEST = "test.record"
PATH_TRAIN = "Dataset/train/"
PATH_RECORD_TRAIN = "train.record"

IMAGE_EXT = ".jpg"
IMAGE_FORMAT = b'jpg'

# This function defines the different classes the dataset has and return a different number per each.
# NEED TO BE UPDATED IN CASE THE DATASET CHANGES
def class_text_to_int(row_label):
    if row_label == 'nut':
        return 1
    if row_label == 'bolt':
        return 2
    else:
        return 3

# Reads the xml and the images, and create the tf records files. 
def xml_to_tf(path_input, path_output):
    xml_list = []
    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']

    writer = tf.compat.v1.python_io.TFRecordWriter(path_output)

    files = os.listdir(path_input)
    for file in files:
        if file.endswith(".xml"):
            xmlFile = path_input + file

            tree = ET.parse(xmlFile)
            root = tree.getroot()
            
            filename = root[1].text
            width = int(root[4][0].text)
            height = int(root[4][1].text)

            xmins = []
            xmaxs = []
            ymins = []
            ymaxs = []
            classes_text = []
            classes = []
            

            for member in root.findall('object'):
                beer = member[0].text
                xmin = int(member[4][0].text)
                ymin = int(member[4][1].text)
                xmax = int(member[4][2].text)
                ymax = int(member[4][3].text)
                
                xmins.append(xmin/width)
                xmaxs.append(xmax/width)
                ymins.append(ymin/height)
                ymaxs.append(ymax/height)
                classes_text.append(beer.encode('utf8'))
                classes.append(class_text_to_int(beer))

            with tf.io.gfile.GFile(os.path.join(path_input, '{}'.format(filename)), 'rb') as fid:
                encoded_jpg = fid.read()
            tf_example = tf.train.Example(features=tf.train.Features(feature={
                'image/height': dataset_util.int64_feature(height),
                'image/width': dataset_util.int64_feature(width),
                'image/filename': dataset_util.bytes_feature(filename.encode('utf8')),
                'image/source_id': dataset_util.bytes_feature(filename.encode('utf8')),
                'image/encoded': dataset_util.bytes_feature(encoded_jpg),
                'image/format': dataset_util.bytes_feature(IMAGE_FORMAT),
                'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
                'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
                'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
                'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
                'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
                'image/object/class/label': dataset_util.int64_list_feature(classes),
            }))
            
            writer.write(tf_example.SerializeToString())
    writer.close()             
    output_path = os.path.join(os.getcwd(), path_output)
    print('Successfully created the TFRecords: {}'.format(output_path))

xml_to_tf(PATH_TEST, PATH_RECORD_TEST)
xml_to_tf(PATH_TRAIN, PATH_RECORD_TRAIN)
```

<br />

# Training
Once a dataset of images has been generated, we can begin training a model. Instead of creating a model's architecture from scratch we can use a premade one called, SSD Inception V2 based off the Tensorflow Object Detection API. Single Shot Detection (SSD) is a type of architecture that focuses on speed of inferencing without sacrificing accuracy. We will perform transfer learning which means we will overwrite the weights in the output layer of the pre-trained model based on the classes we define from our dataset.

<br />

|![image](/assets/images/ssdlayers.jpg)|
|:--:|
|*The SSD architecture with Inception V2 feature extractor blocks*|

<br />

## Package Requirements
Our first step is to create our virtual environment with all the modules and dependencies needed to train the model. 

> - Python 3.6.8
> - Tensorflow 1.14
> - OpenCV 4.1.1
> - Numpy 1.7
> - Absl-py 0.8.1
> - Cython 0.29.13
> - IPython 7.8
> - Lxml 4.4.1
> - Matplotlib 3.1.1
> - Pandas 0.25.1
> - Pillow 6.2
> - Pip 19.2.3
> - Protobuf 3.10
> - Pycocotools 2.0
> - Wheel 0.33.6

## Prepping the Training folder
Once the packages have been downloaded, we must create a folder in *'tensorflow/models/research/object_detection'* called *'training'*. Place the test.record and train.record datasets in this folder. We also need to create a file called labelmap.pbtxt and fill it with the following.


```
item {
  id: 1
  name: 'nut'
}

item {
  id: 2
  name: 'bolt'
}
```

Then download the SSD Inception V2 model files found [here](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz). Place the pipeline.config and all the model.ckpt files in the training folder. These contain the pretrained model with weights based off the Coco dataset which has 90 different objects it classifies. For this project we need to edit the pipeline.config file so num_classes is equal to 2. We also have to edit fine_tune_checkpoint, label_map_path, and input_path (lines 152-172) to point to the paths of the checkpoint files (CLARIFY THE CKPT FILE PATH THING), labelmap file and .record files respectively.

## Initiate Training
Now that we have the pipeline.config, labelmap.pbtxt, train.record, test.record and model.ckpt files in the training folder we can begin training the model. Open the terminal and change the directory to *~/research* and run the following.
```bash
export PYTHONPATH =$PYTHONPATH:pwd:pwd/slim
```
You must do this every time you open a new terminal. To avoid this put that line at the end of your .bashrc file and replace *'pwd'* with the full path to the research folder in tensorflow.

Now change the directory to *~/object_detection* and start the training script by running the following in the terminal.

```bash
python3 model_main.py
--logtostderr
--model_dir=Training/
--pipeline_config_path=Training/pipeline.config
```

<br />

# Deployment

|![image](/assets/images/flowchart.png)|
|:--:|
|*Flowchart of Nut and Bolt Detection Script*|

<br />

## Prepping the Loop
Now that we've successfully trained the object detection model, we can finally put it to use! We will create a script that will perform inferencing using a stereoscopic camera to detect the three-dimensional coordinates of nuts and bolts in realtime. We will be using an Intel Realsense D435 Depth Camera and its corresponding SDK for this project.
### Import Packages 
Before we do anything, we must import the necessary libraries.
```python
import os
import cv2
import numpy as np
import tensorflow as tf
import pyrealsense2 as rs
from utils import visualization_utils as vis_util
from utils import label_map_util
```

<br />

### Initialization
Next, we need to initialize a few things. First, we initialize the two sensors on the camera. One is the infrared depth sensor and the other is the color sensor. 

```python
# Video Dimensions
WIDTH = 848
HEIGHT = 480

# initialize realsense cam
pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.depth, WIDTH, HEIGHT, rs.format.z16, 30)
config.enable_stream(rs.stream.color, WIDTH, HEIGHT, rs.format.bgr8,30)
profile = pipeline.start(config)
depth_sensor = profile.get_device().first_depth_sensor()
depth_scale = depth_sensor.get_depth_scale()
```

<br />

### Loading Trained Model
Then we must load the label map and trained model into memory in order to perform inferencing in real-time.

```python
# current directory
CWD_PATH = os.getcwd()

# Name of directory containing object detection model
MODEL_NAME = 'inceptionV2'

# Path to frozen graph .pb
PATH_TO_CKPT = os.path.join(CWD_PATH, MODEL_NAME, 'frozen_inference_graph.pb')

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH, 'labels', 'labelmap.pbtxt')

# Load the label map
NUM_CLASSES = 2
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

# load TF model into memory
detection_graph = tf.compat.v1.Graph()
with detection_graph.as_default():
    od_graph_def = tf.compat.v1.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    TFSess = tf.compat.v1.Session(graph=detection_graph)
```

<br />

### Defining Inputs and Outputs
Now we must define the variables that will act as the input and outputs of our model. The only input we will have is the array of pixels we receive from each frame of the video. The three outputs we will get from our model are the top-left X,Y pixel coordinates of the bounding boxes, the class of object it detects, and the percent confidence in the object detection.

```python
# Define input and output tensors:
# input tensor is the image
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

# number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')
```

<br />

### Create the Window
Finally we prepare the window that will show the video.

```python
# Initialize framerate calculation
frame_rate_calc = 1
freq = cv2.getTickFrequency()
font = cv2.FONT_HERSHEY_SIMPLEX

# Name the window
WIN_NAME = 'Fermi Detection'

# create window
cv2.namedWindow(WIN_NAME, cv2.WINDOW_AUTOSIZE)
cv2.moveWindow(WIN_NAME, 120, 500)
```

<br />

## Object Detection Loop
Now that we've finished initializing everything, we can work on taking the data from the live video feed and feeding it through our model in order to detect the desired objects in frame.

```python
while(True):
    t1 = cv2.getTickCount()

    # retrieve video feed and its depth and color data
    frames = pipeline.wait_for_frames()
    depth_frame = frames.get_depth_frame()
    color_frame = frames.get_color_frame()
    if not depth_frame or not color_frame:
    	continue
    depth_image = np.asanyarray(depth_frame.get_data())
    color_image = np.asanyarray(color_frame.get_data())
    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

    # get vertices (need to map depth to color)
    pc = rs.pointcloud()
    points = pc.calculate(depth_frame)
    #vtx = np.asanyarray(points.get_vertices())
    #txt = points.get_texture_coordinates()
    #txt2 = np.asanyarray(points.get_texture_coordinates())
    #print(txt2[0])
    #values = get_texcolor(color_frame, txt[0])
    #print(values)

    #  We get a frame from the video, and we expand its dimensions to the tensor shape
    #  [1, None, None, 3]
    frame_expanded = np.expand_dims(color_image, axis=0)

    # We perform the detection of objects, providing the video image as input
    (boxes, scores, classes, num) = TFSess.run(
        [detection_boxes, detection_scores, detection_classes, num_detections],
        feed_dict={image_tensor: frame_expanded})
        <br />
    # Draw the bounding box, class, confidence
    vis_util.visualize_boxes_and_labels_on_image_array(
        color_image,
        np.atleast_2d(np.squeeze(boxes)),
        np.atleast_1d(np.squeeze(classes).astype(np.int32)),
        np.atleast_1d(np.squeeze(scores)),
        category_index,
        use_normalized_coordinates=True,
        line_thickness=8,
        min_score_thresh=0.70)

    intrin = color_frame.profile.as_video_stream_profile().intrinsics

    # retrieve coordinates for every box per frame (sometimes glitches and draws everywhere)
    for box in boxes[0]:
    	if box[0] != 0 and box[1] != 0 and box[2] != 0 and box[3] != 0:
            xmin = int(box[1]*848)
            ymin = int(box[0]*480)
            distance = depth_frame.get_distance(xmin,ymin)
            #dmm = int(distance*1000)
            #cv2.rectangle(color_image,(xmin,ymin-20),(xmin+80,ymin-40),(0,255,0),-1)
            #cv2.putText(color_image,str(dmm)+"mm",(xmin,ymin-20),font,0.4,(255,255,0),2)
            xyvalues = rs.rs2_deproject_pixel_to_point(intrin, [xmin,ymin], distance*depth_scale)
            xvalue = int(xyvalues[0]*1000000)
            yvalue = int(xyvalues[1]*1000000)
            zvalue = int(xyvalues[2]*1000000)
            cv2.putText(color_image,"(X,Y,Z): ("+str(xvalue)+", "+str(yvalue)+", "+str(zvalue)+") mm",(xmin, ymin-20), font,0.4,(255,255,0),2)

    # Calc and draw FPS
    t2 = cv2.getTickCount()
    time1 = (t2-t1)/freq
    frame_rate_calc = 1/time1
    cv2.putText(color_image,"FPS: {0:.2f}".format(frame_rate_calc),(30,50),font,1,(255,255,0),2,cv2.LINE_AA)

    # Update window
    cv2.imshow(WIN_NAME, color_image) 
    # To show both color and depth camera:
    # images = np.hstack((color_image, depth_colormap))
    # cv2.imshow('RealSense', images) 

    # press escape to quit
    if cv2.waitKey(1) == 27:
        print("Exiting...")
        break

pipeline.stop()
cv2.destroyAllWindows()
```

<br />

# Conclusion
The scope of the project was limited by the time and budget given as well as the early semester release due to the COVID-19 quarantine. However, the future work built from the completed prototype looks quite promising. The camera calculates the 3D coordinates of any nut and bolt it sees using the trained model. This easily allows a robotic arm that is connected to our platform to reach the fastener once it is spatially  calibrated to the environment. A high-quality arm with six degrees of freedom and a force feedback gripper was out of our budget and beyond the focus of our project but is necessary for the completion of the task at hand for semi-autonomous maintenance in hazardous environments.
