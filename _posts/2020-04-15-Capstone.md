---
title: "UIC x Fermilab ML Capstone Project"
date: 2020-04-15T15:34:30-04:00
categories:
  - Capstone Project
tags:
  - Machine Learning
  - Python
toc: true
toc_label:
toc_icon: 'bars'
classes: 

---

# Overview
When faced with a business challenge that may require a machine learning solution, we must consider the following. What is the ML (machine learning) problem type? What are the data sources? How will the output be used? What metrics will deem the model successful? How will the finished model be properly implemented into the workflow? What are some possible non-ML solutions? These are some of the questions my collegues, Tisha, Chris, Mohammed, and I had to ask ourselves for our University of Illinois capstone project when we were tasked with exploring the first machine learning implementation at Fermi National Accelerator Labratory (Fermilab). 
## Fermilab's Problem
Fermilab has a particle accelerator that fires high power protons for various experiments. This means that the accelerator itself becomes too radioactive for maintenance workers to easily work on it. They currently need to use hot cells with mechanical arms and PPE to perform maintenance on the accelerator. This process takes a long time and requires extensive training for the workers. Even with PPE the workers are still at risk of accumulating radiation over time so implementing a ML approach to maintenance will not only save Fermilab time and money but also help promote the health and safety of its workers.
## Proposed Solution
### Input to ML Algorithm
The first step of any maintenance work on the accelerator is to remove any nuts and bolts to gain access to the inner parts. Therefore it would be helpful for a robotic arm to automatically remove these fasteners without the need of a human worker. In order to achieve this we need to implement a computer vision algorithm that would be able to locate these fasteners in three-dimensional space. The robot will "see" the fastener using a 3D camera and will calculate the coordinates of said fastener in order to travel that distance. The camera we used is the Intel Realsense D345 which has RGB sensing and stereoscopic depth sensing technology built in along with a robust SDK for scripting purposes. 
### The ML Algorithm
The task of "seeing" an object in a video feed requires using a computer vision solution. There are a number of computer vision technologies that don't utilize machine learning, but they are only applicible in a very uniform environment like a factory's assembly line where a camera wouldn't have much variation in the images it sees. For our situation there are varying angles, lighting conditions, backgrounds, colors, and even types of fasteners on the accelerator. In order to filter through all the unnecessary information to locate a nut or bolt in an image we will use a type of machine learning called object detection. Object detection utilizes a type of machine learning called deep learning which relies on big datasets and powerful hardware for the learning process. We will be supervising the learning process which means we will feed the model large quantities of images of nuts and bolts that are labeled accordingly. This is in contrast to unsupervised learning which lets the algorithm create its own classifications depending on the features it extracts from the dataset. That's not useful for us because we already know what kind of fasteners exist on the accelerator. Deep neural networks have multiple hidden layers between the input and output that extract complex features a human would never be able to find. The model will learn these features specific to a nut and a bolt making it so it shouldn't matter where the camera is positioned or what part of the accelerator the camera is pointed at.
### Output from ML Algorithm
The scope of our project was limited by the time and budget given to us as well as the early semester release due to the COVID-19 quarantine. However the future work built from our completed prototype looks quite promising. Our 3D camera using our trained model calculates the 3D coordinates of any nut and bolt it sees. This easily allows a robotic arm that is connected to our platform to reach the fastener once it is spacially calibrated to the environment. A high quality arm with six degrees of freedom and a force feedback gripper was out of our budget and beyond the focus of our project, but is necessary for the completion of the task at hand for semi-autonomous maintenance in hazardous environments.

# Assumptions
Before moving to the design process and design of experiments of the projects, assumptions must be established so as to narrow the scope of the project. By setting limits to the extent to which the project is conducted, a higher success rate for the project is established for the given timeline. The following section describes the assumptions that are made in regard to machine learning for semi-autonomous maintenance in a hazardous environment.
	The first assumption made is that the device will be first be comprised of a microcontroller in order to fulfill the design requirements of being mountable on a robotic arm and capable of enclosing all necessary software. It must have enough storage capacity to house all necessary libraries and packages that will be used to create a machine learning and computer vision system. The two microcontrollers that will be considered for the project are the Jetson Nano and Jetson TX2. Comparisons of these devices are found in the Proposed Design Solutions in this report.  
Another assumption for the project is that the camera to be used is a 3D stereo camera. This assumption is made due to the design requirement that the device must be able to determine the location and orientation of the previously identified fasteners to a common coordinate origin – which cannot be determined using a 2D camera. Furthermore, the underlying principle of deep learning and using CNNs is based on the human brain and human experience and the human experience is based on a three-dimensional reality [10]. Therefore, having a 3D stereo camera collecting data on the environment would reflect a more accurate image of the camera’s surroundings and allow for the creation of a virtual environment to base a coordinate system about which a robotic arm can be adjusted to move towards the nuts and bolts detected by the camera in 3D space [10]. It can be concluded that by using both CNNs and 3D stereo camera, the vision system would be able to create a more accurate model of reality which would then allow the entire system to be more accurate. Another assumption is to be taken that while programming the robotic arm, the camera must be set to lie within this range of distance from the targeted nuts and bolts on the bolt circle.
Since the goal of the machine learning device is to perform maintenance and end-of-life procedures on highly radioactive components from an accelerator beamline enclosure, it can be assumed that the device attached to the robot arm will only be used once before being becoming obsolete and having to be thrown away with all other contaminated parts of the beamline and maintenance devices. Therefore, the device, software and hardware must be easily reproducible for future maintenance projects of similar scope. This can be done so by backing up the system to a separate hard drive that can be uploaded onto other Nvidia Jetson devices as the project uses various scripts and programs that are not unique to the device that will be presented in this project. 
	Finally, transfer learning will be applied to the machine learning process for the first test at creating the machine learning system. Since many models have already been created and published for tasks of relatively similar scopes to this project, a model will be chosen based on accuracy and an optimized time for training the model, and transfer learning will be applied by means of creating a unique dataset of nuts and bolts and training the premade model to detect the objects presented in our project’s dataset.

# Final Design
After extensive research it can be determined that though many different machine learning applications exist in the realm of image classification, object detection and image segmentation, nothing has yet to be applied to the specific task and environment for this project. From this, a decision matrix, Table 1, was created for choosing between the Jetson Nano and the Jetson TX2 as the microcontroller for the project. The memory capacity as well as compatibility with the two provided 3D stereo cameras were given the highest scores of 5 as these are key for moving forward with the project. The storage capacity was given a score of 3 as memory is more vital to the project compared to storage. The cost was given a low score of 2 as both devices were provided either by UIC or by and Fermilab. The size was given a score of 4 as Fermilab wants the device to be able to be connected to a robotic arm.

<br />

|![image](/assets/images/table1.png)|
|:--:|
|*Table 1: Decision Matrix for microcontroller*|

<br />

Next a decision matrix, Table 2, was made to compare the different types of cameras that may be used for the object detection. The resolution was given the highest score of 5 as this is most important when having the system detect the target objects. The cost was given a score of 2 as the cameras were all provided by either Fermilab or fell within the budget for UIC for the project. Compatibility was given a score of 3 because the camera chosen must be compatible with the Jetson interface, however, most cameras are compatible, so this was not a problem to overcome. The size was also given a score of 3 as the camera will be attached to a robot arm and must be able to move with the robot arm without inhibiting the motion

<br />

|![image](/assets/images/table2.png)|
|:--:|
|*Table 2: Decision Matrix for camera*|

<br />

Next a decision matrix, Table 3, was made to compare the different types of model architectures that may be used for training the custom machine learning model using transfer learning. The average recall and precision were given the highest scores of 10 each as these are equally important when having the system correctly detect and differentiate between the target objects. The speed of detected the objects was given a score of 5 as this was subject to if a human eye may notice a notable difference.

<br />

|![image](/assets/images/table3.png)|
|:--:|
|*Table 3: Decision Matrix for model architecture*|

<br />

Therefore, taking the above assumptions and decision matrices scores into consideration, this leads to the following proposed design of the system for our project. The SSD Inception V2 model architecture will be used to train the custom model, while the Nvidia Jetson Nano will be used as the microcontroller for running the pretrained object detection and classification script via the Intel Realsense D435 depth camera. However, as the Jetson Nano does not have the RAM compacity to perform the training of the model, a local directory on the supercomputer, Dragon, will be set up to be used to train our model using transfer learning. A custom object and coordinate detection script will be created for detecting nuts and bolts on a flange and displaying the 3D coordinate location of each target object.

# Training
Once a dataset of images has been generated, we can begin training a model. Instead of creating a model's architecture from scratch we can use a premade one called, SSD Inception V2 based off of the Tensorflow Object Detection API. Single Shot Detection (SSD) is a type of architecture that focuses on speed of inferencing without sacrificing accuracy. We will perform transfer learning which means we will overwrite the weights in the output layer of the pre-trained model based on the classes we define from our dataset.

<br />

|![image](/assets/images/ssdlayers.jpg)|
|:--:|
|*The SSD architecture with Inception V2 feature extractor blocks*|

<br />

## Package Requirements
Our first step is to create our virtual environment with all the modules and dependecies needed to train the model. 

> - Python 3.6.8
> - Tensorflow 1.14
> - OpenCV 4.1.1
> - Numpy 1.7
> - Absl-py 0.8.1
> - Cython 0.29.13
> - IPython 7.8
> - Lxml 4.4.1
> - Matplotlib 3.1.1
> - Pandas 0.25.1
> - Pillow 6.2
> - Pip 19.2.3
> - Protobuf 3.10
> - Pycocotools 2.0
> - Wheel 0.33.6

## Prepping the Training folder
Once the packages have been downloaded we must create a folder in *'tensorflow/models/research/object_detection'* called *'training'*. Place the test.record and train.record datasets in this folder. We also need to create a file called labelmap.pbtxt and fill it with the following.


```
item {
  id: 1
  name: 'nut'
}

item {
  id: 2
  name: 'bolt'
}
```

Then download the SSD Inception V2 model files found [here](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz). Place the pipeline.config and all the model.ckpt files in the training folder. These contain the pretrained model with weights based off of the Coco dataset which has 90 different objects it classifies. For this project we need to edit the pipeline.config file so num_classes is equal to 2. We also have to edit fine_tune_checkpoint, label_map_path, and input_path (lines 152-172) to point to the paths of the checkpoint files (CLARIFY THE CKPT FILE PATH THING), labelmap file and .record files respectively.

<br />

## Training the model
Now that we have the pipeline.config, labelmap.pbtxt, train.record, test.record and model.ckpt files in the training folder we can begin training the model. Open up the terminal and change the directory to *~/research* and run the following.
```bash
export PYTHONPATH =$PYTHONPATH:pwd:pwd/slim
```
You must do this everytime you open up a new terminal. To avoid this put that line at the end of your .bashrc file and replace *'pwd'* with the full path to the research folder in tensorflow.

<br />

Now change the directory to *~/object_detection* and start the training script by running the following in the terminal.

```bash
python3 model_main.py
--logtostderr
--model_dir=Training/
--pipeline_config_path=Training/pipeline.config
```

# Deploying Trained Model
## Prepping the Loop
### Import Packages
Now that we've successfully trained the object detection model we can finally put it to use! We will create a script that will perform inferencing using a stereoscopic camera to detect the three dimensional coordinates of nuts and bolts in realtime. We will be using an Intel Realsense D435 Depth Camera and its corresponding SDK for this project. 
Before we do anything we must import the necessary libraries.
```python
import os
import cv2
import numpy as np
import tensorflow as tf
import pyrealsense2 as rs
from utils import visualization_utils as vis_util
from utils import label_map_util
```

<br />

### Initialization
Next we need to intitialize a few things. First we initialize the two sensors on the camera. One is the infrared depth sensor and the other is the color sensor. 

```python
# Video Dimensions
WIDTH = 848
HEIGHT = 480

# initialize realsense cam
pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.depth, WIDTH, HEIGHT, rs.format.z16, 30)
config.enable_stream(rs.stream.color, WIDTH, HEIGHT, rs.format.bgr8,30)
profile = pipeline.start(config)
depth_sensor = profile.get_device().first_depth_sensor()
depth_scale = depth_sensor.get_depth_scale()
```

<br />

### Loading Trained Model
Then we have to load the label map and trained model into memory in order to perform inferencing in realtime.

```python
# current directory
CWD_PATH = os.getcwd()

# Name of directory containing object detection model
MODEL_NAME = 'inceptionV2'

# Path to frozen graph .pb
PATH_TO_CKPT = os.path.join(CWD_PATH, MODEL_NAME, 'frozen_inference_graph.pb')

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH, 'labels', 'labelmap.pbtxt')

# Load the label map
NUM_CLASSES = 2
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

# load TF model into memory
detection_graph = tf.compat.v1.Graph()
with detection_graph.as_default():
    od_graph_def = tf.compat.v1.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    TFSess = tf.compat.v1.Session(graph=detection_graph)
```

<br />

### Defining Inputs and Outputs
Now we must define the variables that will act as the input and outputs of our model. The only input we will have is the array of pixels we recieve from each frame of the video. The three outputs we will get from our model are the top-left X,Y pixel coordinates of the bounding boxes, the class of object it detects, and the percent confidence in the object detection.

```python
# Define input and output tensors:
# input tensor is the image
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

# number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')
```

<br />

### Create the Window
Finally we prepare the window that will show the video.

```python
# Initialize framerate calculation
frame_rate_calc = 1
freq = cv2.getTickFrequency()
font = cv2.FONT_HERSHEY_SIMPLEX

# Name the window
WIN_NAME = 'Fermi Detection'

# create window
cv2.namedWindow(WIN_NAME, cv2.WINDOW_AUTOSIZE)
cv2.moveWindow(WIN_NAME, 120, 500)
```

<br />

## Object Detection
Now that we've finished initializing everything, we can work on taking the data from the live video feed and feeding it through our model in order to detect the desired objects in frame.

```python
while(True):
    t1 = cv2.getTickCount()

    # retrieve video feed and its depth and color data
    frames = pipeline.wait_for_frames()
    depth_frame = frames.get_depth_frame()
    color_frame = frames.get_color_frame()
    if not depth_frame or not color_frame:
    	continue
    depth_image = np.asanyarray(depth_frame.get_data())
    color_image = np.asanyarray(color_frame.get_data())
    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

    # get vertices (need to map depth to color)
    pc = rs.pointcloud()
    points = pc.calculate(depth_frame)
    #vtx = np.asanyarray(points.get_vertices())
    #txt = points.get_texture_coordinates()
    #txt2 = np.asanyarray(points.get_texture_coordinates())
    #print(txt2[0])
    #values = get_texcolor(color_frame, txt[0])
    #print(values)

    #  We get a frame from the video, and we expand its dimensions to the tensor shape
    #  [1, None, None, 3]
    frame_expanded = np.expand_dims(color_image, axis=0)

    # We perform the detection of objects, providing the video image as input
    (boxes, scores, classes, num) = TFSess.run(
        [detection_boxes, detection_scores, detection_classes, num_detections],
        feed_dict={image_tensor: frame_expanded})
        <br />
    # Draw the bounding box, class, confidence
    vis_util.visualize_boxes_and_labels_on_image_array(
        color_image,
        np.atleast_2d(np.squeeze(boxes)),
        np.atleast_1d(np.squeeze(classes).astype(np.int32)),
        np.atleast_1d(np.squeeze(scores)),
        category_index,
        use_normalized_coordinates=True,
        line_thickness=8,
        min_score_thresh=0.70)

    intrin = color_frame.profile.as_video_stream_profile().intrinsics

    # retrieve coordinates for every box per frame (sometimes glitches and draws everywhere)
    for box in boxes[0]:
    	if box[0] != 0 and box[1] != 0 and box[2] != 0 and box[3] != 0:
            xmin = int(box[1]*848)
            ymin = int(box[0]*480)
            distance = depth_frame.get_distance(xmin,ymin)
            #dmm = int(distance*1000)
            #cv2.rectangle(color_image,(xmin,ymin-20),(xmin+80,ymin-40),(0,255,0),-1)
            #cv2.putText(color_image,str(dmm)+"mm",(xmin,ymin-20),font,0.4,(255,255,0),2)
            xyvalues = rs.rs2_deproject_pixel_to_point(intrin, [xmin,ymin], distance*depth_scale)
            xvalue = int(xyvalues[0]*1000000)
            yvalue = int(xyvalues[1]*1000000)
            zvalue = int(xyvalues[2]*1000000)
            cv2.putText(color_image,"(X,Y,Z): ("+str(xvalue)+", "+str(yvalue)+", "+str(zvalue)+") mm",(xmin, ymin-20), font,0.4,(255,255,0),2)

    # Calc and draw FPS
    t2 = cv2.getTickCount()
    time1 = (t2-t1)/freq
    frame_rate_calc = 1/time1
    cv2.putText(color_image,"FPS: {0:.2f}".format(frame_rate_calc),(30,50),font,1,(255,255,0),2,cv2.LINE_AA)

    # Update window
    cv2.imshow(WIN_NAME, color_image) 
    # To show both color and depth camera:
    # images = np.hstack((color_image, depth_colormap))
    # cv2.imshow('RealSense', images) 

    # press escape to quit
    if cv2.waitKey(1) == 27:
        print("Exiting...")
        break

pipeline.stop()
cv2.destroyAllWindows()
```
